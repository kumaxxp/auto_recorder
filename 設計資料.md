# 📦 **設計資料：PC用 Segmentation UI + 低速走行探索システム**

### ※ Phase1 実装対象（走行制御は接続だけ、実走は後）

---

## 🎯 **目的**

* Jetson または PC 上で動作するプロトタイプを作成
* カメラ映像をリアルタイム表示（640×480）
* 魚眼補正を適用
* SLIC superpixel segmentation の粒度を UI スライダで変更
* segmentation された領域をクリックして **3分類**

  * `DRIVABLE` / `BLOCKED` / `UNKNOWN`
* 走行判定用の ROI 水平ラインをドラッグで変更可能
* セグメンテーション結果に基づいて走行可能率や左右判定の debug 値を表示

---

## 🧠 **システム構成**

```
Camera Stream (640x480)
    ↓
Fisheye Undistort  (OpenCV fisheye)
    ↓
SLIC Superpixel segmentation  (skimage or OpenCV)
    ↓
Overlay visualization + mask
    ↓
NiceGUI Web UI (PC browser)
    ↑   ↑    ↑
 mouse-click   slider   ROI-line drag
```

---

## 👨‍💻 **必要な Python ライブラリ**

| ライブラリ               | 用途                          |
| ------------------- | --------------------------- |
| opencv-python       | カメラ取得 / undistort / overlay |
| numpy               | 画像処理                        |
| scikit-image        | SLIC segmentation           |
| nicegui             | WebUI                       |
| jetracer (optional) | 走行制御統合の準備                   |

---

## 📺 **UI 仕様（PC用）**

### 画面レイアウト

```
┌──────────────────────────── Camera + Segmentation Overlay ─────────────────────────────┐
│ [640x480] 画像と superpixel, mask overlay                                                │
│ ROI horizontal line (ドラッグで上下移動)                                               │
└──────────────────────────────────────────────────────────────────────────────────────────┘

┌────────── Controls ──────────┐   ┌──────── Debug ────────┐
│ Superpixel granularity slider (10-500) │   │ Drivable %              │
│ Class buttons: [DRIVABLE] [BLOCKED] [UNKNOWN] │   │ Left/Right ratio        │
│ Mode display                             │   │ FPS                      │
└──────────────────────────────────────────┘   └─────────────────────────┘
```

---

## 🖱️ **ユーザー操作イベント**

| 操作         | 動作                                  |
| ---------- | ----------------------------------- |
| マウスクリック    | クリック位置の superpixel ID を取得し、選択クラスを付与 |
| 粒度スライダ変更   | SLIC segmentation を再実行して再描画         |
| ROIラインドラッグ | y_cutline 値を変更／表示更新                 |
| クラスボタン     | クリック時に指定クラス付与モード変更                  |

---

## 🧱 **データ構造**

### segmentation 結果格納

```
segments: 2D array same size as image, value = superpixel ID
label_map: dict {superpixel_id: class_enum}
class_enum: {0: UNKNOWN, 1: DRIVABLE, 2: BLOCKED}
```

### ROI判定

```
ROI = segments[y_cutline : 480, :]
drivable_ratio = % labeled DRIVABLE within ROI
left_ratio = ratio left half vs right half
```

---

## 📡 **低速走行決定ロジック**

（現段階では debug テキスト表示のみ）

```
if drivable_ratio > threshold:
    action = "FORWARD"
elif left_ratio > 0:
    action = "TURN_LEFT"
else:
    action = "TURN_RIGHT"
if drivable_ratio < stop_threshold:
    action = "STOP"
```

---

## 🪪 **クラス定義案**

```python
class SegmentLabel(Enum):
    UNKNOWN = 0
    DRIVABLE = 1
    BLOCKED = 2
```

---

## 🚗 **走行制御（後で統合）**

JetRacer Adapter を別ファイルに用意

```python
class VehicleController:
    def set_throttle(self, value: float):
        pass
    def set_steering(self, value: float):
        pass
```

---

## 🚀 **Phase 2: DeepStream 統合とデュアルカメラ対応 (2025/11/23)**

### 変更の概要
初期のCPUベース(SLIC)の手法から、JetsonのGPUを活用するDeepStreamパイプラインへ移行しました。また、前方(Top)と足元(Bottom)の2つのカメラを同時に処理する構成に変更しました。

### 🛠️ **システム構成の更新**

```
[Camera 0 & 1] (CSI/USB)
    ↓
[GStreamer Pipeline]
    ├── nvarguscamerasrc (Capture)
    ├── nvstreammux (Batching)
    ├── nvinfer (Segmentation Inference / ResNet-UNet)
    │      └── models/drivable_area.onnx
    ├── nvsegvisual (Mask Visualization)
    └── appsink (Output to Python)
         ├── RGB Frame (Video)
         └── Mask Frame (Segmentation Result)
    ↓
[Python / NiceGUI]
    ├── CameraStream (Pipeline Management)
    ├── Overlay Blending (OpenCV)
    └── Dashboard (Dual View)
```

### ⚙️ **重要な設定・調整項目**

#### 1. DeepStream パイプライン設定
*   **解像度整合**: モデルの入力サイズ（256x256）とカメラ出力（640x480）の不一致を解消するため、パイプライン内でリサイズ処理を明示的に行っています。
*   **アスペクト比**: `configs/deepstream_drivable_segmentation.txt` にて `maintain-aspect-ratio=0` を設定。これにより、推論結果のマスクが画面全体に引き伸ばされ、グレーのパディング（余白）が発生するのを防いでいます。

#### 2. オーバーレイ合成ロジック (`ui.py`)
*   **ゴースト除去**: `nvsegvisual` の出力をそのまま使うと、背景画像が二重になる現象が発生しました。
*   **解決策**:
    1.  DeepStreamからは「マスク画像（背景黒＋色付きマスク）」のみを取得。
    2.  Python側で、マスク画像から背景色（最頻色）を特定し、それ以外の領域を「有効なマスク」として抽出。
    3.  `cv2.addWeighted` を使用して、生のカメラ映像にマスクを半透明で合成。

#### 3. デュアルカメラ構成
*   `main.py` にて `CameraStream(index=0)` と `CameraStream(index=1)` をインスタンス化。
*   NiceGUI上で縦に並べて表示し、それぞれのカメラ映像とセグメンテーション結果を確認可能にしました。

### 📦 **追加された依存関係**
*   **DeepStream 7.1**: 推論エンジンのバックエンド
*   **GStreamer Python Bindings (`gi`)**: PythonからGStreamerパイプラインを制御するために必須
*   **ONNX Model**: `drivable_area.onnx` (走行可能領域分割モデル)

---

## 🎨 **Phase 3: Cityscapes データセットを用いた多クラスセグメンテーションの検討**

### 概要
ユーザーからの要望により、[jetson-inferenceの事例](https://www.souichi.club/deep-learning/semantic-segmentation/)と同様の、Cityscapesデータセットを用いたリッチなセグメンテーションへの移行を検討します。現在は「走行可能領域(Drivable Area)」のみの2値分類ですが、Cityscapesモデルを導入することで、車、歩行者、道路、建物などを個別に認識可能になります。

### 📊 **Cityscapes モデルの特長とメリット**
*   **多クラス認識**: 道路(Road)だけでなく、歩道(Sidewalk)、車(Car)、人(Person)、信号(Traffic Light)など約19〜30クラスを識別可能。
*   **状況判断の向上**: 単に「通れるか」だけでなく、「なぜ通れないか（車がいるから？壁だから？）」を判断材料にできる。
*   **視覚効果**: 画面上がカラフルに色分けされ、デモとしての見栄えが大幅に向上する。

### 🛠️ **実装に向けた変更点**

#### 1. モデルの変更
*   現在の `drivable_area.onnx` (2クラス) から、`fcn-resnet18-cityscapes` や `unet-cityscapes` (19クラス以上) のONNXモデルへ変更が必要です。
*   NVIDIA NGC (NVIDIA GPU Cloud) から提供されている `Semantic Segmentation` 事前学習済みモデル（PeoplesSemSegNetなど）や、`jetson-inference` のモデルをONNX変換して利用することを想定します。

#### 2. DeepStream 設定 (`deepstream_app_config`)
*   `num-detected-classes`: 2 → 19 (またはモデルのクラス数) に変更。
*   `labelfile-path`: Cityscapes用のラベル定義ファイル（`labels.txt`）を作成・指定。
*   `segmentation` セクション: 各クラスに対応するオーバーレイ色 (`overlay-color0` ~ `overlay-color18`) を定義。

#### 3. UI / ロジック (`ui.py`)
*   **現在のロジック**: 「背景以外＝走行可能」という簡易的な判定。
*   **変更後のロジック**:
    *   特定のクラスID（例: `Road=0`）を「走行可能」として抽出。
    *   特定のクラスID（例: `Car=11`, `Person=12`）を「障害物」として警告表示。
    *   オーバーレイ表示は、単色（緑）ではなく、DeepStreamが出力するマルチカラーのマスクをそのまま（あるいは半透明で）表示するように変更。

### ⚠️ **懸念事項と対策**
*   **処理負荷**: クラス数が増えることで推論負荷が上がり、FPSが低下する可能性があります。
    *   *対策*: 入力解像度を調整する（例: 512x256）、または軽量なモデル（MobileNetバックボーンなど）を選定する。
*   **色の取り扱い**: 現在の「背景色除去ロジック」は使えなくなるため、DeepStream側で適切な色設定を行うか、Python側でクラスIDマップ（`mask_classes`）を直接解析する必要があります。

### 📝 **実施判断**
この設計に基づき、Cityscapesモデルへの移行を実施するかどうかを決定します。
実施する場合、まずは適切なONNXモデルの入手から開始します。

---

## ⚡ **Phase 4: パフォーマンス検証と運用オプション (2025/11/24)**

### 🔍 フェーズA〜C 最適化の要約
| Phase | 目的 | 主な変更 | 実測FPS |
| --- | --- | --- | --- |
| A | UI/Appsink起因のボトルネック切り分け | セグメンテーション無効・生フレーム表示で計測 | 3.3 FPS（UIタイマー10Hz＋1/3間引きの理論上限） |
| B | DeepStreamカラーオーバーレイ復帰 | `PHASE_A_RAW_PREVIEW=False` に戻しつつUIのみ簡素化 | 6.0 FPS |
| C | UIと処理スレッドの分離 | 背景ワーカー＋NiceGUIタイマー、JPEG間引き0化 | 6.6 FPS（タイマー20Hz）→ 29.8 FPS（タイマー33ms） |

### 🧠 技術ポイント
- **バックグラウンド処理スレッド**: `SegmentationDashboard` 内に専用Workerを追加し、DeepStream結果をキューイング。UIスレッドは最新完成フレームのみ受信するため、`ui.timer` 周期に依存せず処理継続可能。
- **カラーLUT変換の高速化**: `segmentation.py` の `mask_to_segments` を24bit LUT方式に刷新し、CityscapesのBGRコード→クラスID変換を1メモリアクセスで完了。`[PhaseC worker]` が ~48 FPS で動作するまで高速化。
- **UIタイマー 30Hz**: `UI_REFRESH_INTERVAL_SEC = 1/30` に設定。NiceGUIのJPEG/Base64配信上限に到達し、安定して 29〜30 FPS 表示を達成。

### 🖥️ CLI オプション (`main.py`)
| オプション | 役割 | 使用例 |
| --- | --- | --- |
| `--duration <秒>` | 指定秒数後に自動 `app.shutdown()`。短時間ベンチや無人テスト向け。 | `./.venv/bin/python main.py --duration 5` |
| `--no-browser` | NiceGUIの自動ブラウザ起動を抑止。既存タブを使い回せる。 | `./.venv/bin/python main.py --no-browser` |

### ✅ 現状まとめ
- DeepStream経路・Cityscapesモデルを維持したまま、UI表示で ~30 FPS を達成。
- 更なるFPS向上には、NiceGUIのJPEG/HTTPストリームの置き換え（WebRTC/RTSP等）が必要。今後の検討項目に追加。

---

## 🚦 **Phase 5: 30FPS超を狙う映像配信戦略**

### 🎯 課題整理
- NiceGUI標準の `interactive_image` は、`image/jpeg + base64 + websocket` という構成のため 720p 以上では 30FPS 前後が物理的限界。
- ブラウザ側で複数タブを開くとすべてが同じJPEGストリームを受信するため帯域負荷が線形に増大。
- Jetson側では DeepStream が既に NVMM 上で RGBA フレームを保持しているため、GPU→CPUコピーを避けつつストリーミングできれば 60FPS も現実的。

### 🔁 代替案の比較
| 方針 | 長所 | 想定FPS / レイテンシ | 実装コスト | メモ |
| --- | --- | --- | --- | --- |
| 1. **NiceGUI + バイナリWebSocket** | 既存UIを流用。`canvas` へ `Uint8Array` で描画すれば30FPS超が可能。 | 40〜45FPS（720p想定） / ~80ms | 中 | NiceGUIの`run_javascript`と`ui.add_head_html`で受信処理を注入。サーバ側は`websocket`モジュールで `application/octet-stream` をpush。 |
| 2. **MJPEG / RTSP (GStreamer)** | `appsrc ! jpegenc ! multipartmux` や `appsrc ! x264enc ! rtph264pay` でHTTP/RTSP配信。既存の監視ツール等で視聴可能。 | 30〜60FPS / 100〜150ms | 中〜高 | NiceGUIとは別ポートで配信し、UIは `<img src>` や `<video>` 埋め込みで参照。マルチクライアントでも一つのmuxから配信可能。 |
| 3. **WebRTC (GStreamer webrtcbin / aiortc)** | ブラウザネイティブ、低遅延（<80ms）、60FPS以上。音声やデータチャネルも拡張可。 | 60FPS+ / <80ms | 高 | Jetsonで `webrtcbin` を使い NVMM→VP9/H264 encode→SRTP送信。NiceGUIとは別にsignaling（WebSocket）実装が必要。 |
| 4. **OpenCV HighGUI / PyQt ローカル表示** | 端末ローカルで60FPS超。ただしリモート閲覧不可。 | 60FPS+ / <30ms | 低 | デバッグ専用。ブラウザ要件が無い場合は最速。 |

### 🧭 推奨ロードマップ
1. **短期**: 方針1（バイナリWebSocket）で NiceGUI 内の `interactive_image` を `canvas` 描画へ置き換え、既存UIの操作系を維持したまま 40FPS超を目指す。実装ステップは以下。
    - サーバ: 背景ワーカーから `cv2.imencode('.jpg', ...)` の代わりに `frame.tobytes()` (BGR/PNG) を送信。
    - クライアント: `WebSocket` 受信で `ImageBitmap` に変換し `<canvas>` へ描画。`requestAnimationFrame` で同期。
2. **中期**: 方針2の MJPEG/RTSP を併設し、NiceGUIは制御UIに専念。HLS/RTSPクライアントを埋め込めばブラウザ一枚で完結。
3. **長期**: 方針3の WebRTC 化を検討。将来的に遠隔操縦や多拠点配信を想定するならここまで行う価値がある。

今後は上記ロードマップに沿って PoC を順番に実施し、実測FPSと遅延を比較する。

## ✅ **Phase C: ROIベース判定とUIスロットリング完了レポート (2025/11/24)**

- **ROI/走行判定**: Cityscapesの `road` / `sidewalk` (class id 3,4) のみを走行可能とみなし、`ROI_Y_RATIO=0.35` を初期ラインに採用。右ドラッグで `(manual)` に切り替えると自動調整を停止し、以降のフレームでもユーザー指定位置を保持します。
- **GO / AVOID / STOP**: ROI 内の走行率で `>12% = GO`, `1〜12% = AVOID`, `<1% = STOP` を決定。ラベルには `Status: GO | drivable=0.63 | L-R=+0.18` のように左右バランスも併記し、色は `STATUS_COLORS` に合わせて更新します。
- **左右バランスとステアリングヒント**: ROI を左右に二分し、それぞれの走行率差分 (L-R) が `±0.05` を超えた場合に `steer=LEFT/RIGHT` をデバッグ欄へ表示。操舵決定の参考値として活用できます。
- **UIスロットリング**: `DISPLAY_TARGET_FPS=10` を時間基準のゲートに使用し、JPEGエンコードが100ms以上かかった場合でも追加描画を抑制。処理スレッドはキュー分離で約50FPSを維持します。

### 🧪 実行ログ (`./.venv/bin/python main.py --duration 20 --no-browser`)
| 指標 | 実測値 | メモ |
| --- | --- | --- |
| `PhaseC worker` | 45〜52 FPS | DeepStream + Cityscapes 推論スレッド。ROI計算を含めても50FPS近辺を維持。 |
| `PhaseC display` | 8.0 FPS | 10FPS上限に対し、JPEG+base64転送のオーバーヘッドで実効8FPS前後。無駄な描画は抑制済み。 |

### 🔄 確認手順
1. `./.venv/bin/python main.py --duration 20 --no-browser`
2. ブラウザでROIラインを右ドラッグし `(manual)` 表示へ切り替え。
3. 路面/障害物を映し替え、`Status` ラベルと `steer` 表示が閾値に従って即時に切り替わることを確認。

### ⚠️ 残課題
- 表示FPSを10に近づけるには JPEG品質/解像度調整、または Phase5 で検討中の WebRTC・MJPEG 化が有効。
- `left_right_ratio` を車体制御へ接続するアダプタ層は未実装。次フェーズで `VehicleController` との統合を行う。

